{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = pd.read_json('quotes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't cry because it's over, smile because it ...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>[attributed-no-source, cry, crying, experience...</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Don't cry because it's over, smile because it ...</td>\n",
       "      <td>Dr. Seuss</td>\n",
       "      <td>[attributed-no-source, cry, crying, experience...</td>\n",
       "      <td>0.155666</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm selfish, impatient and a little insecure. ...</td>\n",
       "      <td>Marilyn Monroe</td>\n",
       "      <td>[attributed-no-source, best, life, love, mista...</td>\n",
       "      <td>0.129122</td>\n",
       "      <td>truth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48386</th>\n",
       "      <td>In Buddhism, they say attachment to anything o...</td>\n",
       "      <td>Jason Mraz</td>\n",
       "      <td>[Suffering, Laugh, Stage]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48387</th>\n",
       "      <td>I love British humor. It's just so - surreal.</td>\n",
       "      <td>Beck</td>\n",
       "      <td>[Love, British, Surreal]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48388</th>\n",
       "      <td>I've got a sense of humor. I'm a funny guy.</td>\n",
       "      <td>Daryl Hall</td>\n",
       "      <td>[Funny, Guy]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48389</th>\n",
       "      <td>Humor is such a wonderful thing, helping you r...</td>\n",
       "      <td>Lynda Barry</td>\n",
       "      <td>[Time, Beautiful, Fool]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48390</th>\n",
       "      <td>Life Is Full of Obstacles, Stumble Upon !!</td>\n",
       "      <td>Pratik Shelar</td>\n",
       "      <td>[inspirational-quotes ]</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>inspiration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48391 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Quote          Author  \\\n",
       "0      Don't cry because it's over, smile because it ...       Dr. Seuss   \n",
       "1      Don't cry because it's over, smile because it ...       Dr. Seuss   \n",
       "2      I'm selfish, impatient and a little insecure. ...  Marilyn Monroe   \n",
       "3      I'm selfish, impatient and a little insecure. ...  Marilyn Monroe   \n",
       "4      I'm selfish, impatient and a little insecure. ...  Marilyn Monroe   \n",
       "...                                                  ...             ...   \n",
       "48386  In Buddhism, they say attachment to anything o...      Jason Mraz   \n",
       "48387      I love British humor. It's just so - surreal.            Beck   \n",
       "48388        I've got a sense of humor. I'm a funny guy.      Daryl Hall   \n",
       "48389  Humor is such a wonderful thing, helping you r...     Lynda Barry   \n",
       "48390         Life Is Full of Obstacles, Stumble Upon !!   Pratik Shelar   \n",
       "\n",
       "                                                    Tags  Popularity  \\\n",
       "0      [attributed-no-source, cry, crying, experience...    0.155666   \n",
       "1      [attributed-no-source, cry, crying, experience...    0.155666   \n",
       "2      [attributed-no-source, best, life, love, mista...    0.129122   \n",
       "3      [attributed-no-source, best, life, love, mista...    0.129122   \n",
       "4      [attributed-no-source, best, life, love, mista...    0.129122   \n",
       "...                                                  ...         ...   \n",
       "48386                          [Suffering, Laugh, Stage]    0.000000   \n",
       "48387                           [Love, British, Surreal]    0.000000   \n",
       "48388                                       [Funny, Guy]    0.000000   \n",
       "48389                            [Time, Beautiful, Fool]    0.000000   \n",
       "48390                            [inspirational-quotes ]   -0.000001   \n",
       "\n",
       "          Category  \n",
       "0             life  \n",
       "1        happiness  \n",
       "2             love  \n",
       "3             life  \n",
       "4            truth  \n",
       "...            ...  \n",
       "48386        humor  \n",
       "48387        humor  \n",
       "48388        humor  \n",
       "48389        humor  \n",
       "48390  inspiration  \n",
       "\n",
       "[48391 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Popularity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>motivation</th>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friendship</th>\n",
       "      <td>655</td>\n",
       "      <td>655</td>\n",
       "      <td>655</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>893</td>\n",
       "      <td>893</td>\n",
       "      <td>893</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>religion</th>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>957</td>\n",
       "      <td>957</td>\n",
       "      <td>957</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poetry</th>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>994</td>\n",
       "      <td>994</td>\n",
       "      <td>994</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faith</th>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romance</th>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "      <td>1025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arts</th>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knowledge</th>\n",
       "      <td>1052</td>\n",
       "      <td>1052</td>\n",
       "      <td>1052</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happiness</th>\n",
       "      <td>1057</td>\n",
       "      <td>1057</td>\n",
       "      <td>1057</td>\n",
       "      <td>1057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>writing</th>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soul</th>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books</th>\n",
       "      <td>1065</td>\n",
       "      <td>1065</td>\n",
       "      <td>1065</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>1071</td>\n",
       "      <td>1071</td>\n",
       "      <td>1071</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truth</th>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "      <td>1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purpose</th>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "      <td>1095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wisdom</th>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>1130</td>\n",
       "      <td>1130</td>\n",
       "      <td>1130</td>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quotes</th>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "      <td>1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>1397</td>\n",
       "      <td>1397</td>\n",
       "      <td>1397</td>\n",
       "      <td>1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hope</th>\n",
       "      <td>2123</td>\n",
       "      <td>2123</td>\n",
       "      <td>2123</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>success</th>\n",
       "      <td>2175</td>\n",
       "      <td>2175</td>\n",
       "      <td>2175</td>\n",
       "      <td>2175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>philosophy</th>\n",
       "      <td>2504</td>\n",
       "      <td>2504</td>\n",
       "      <td>2504</td>\n",
       "      <td>2504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>3751</td>\n",
       "      <td>3751</td>\n",
       "      <td>3751</td>\n",
       "      <td>3751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>humor</th>\n",
       "      <td>3811</td>\n",
       "      <td>3811</td>\n",
       "      <td>3811</td>\n",
       "      <td>3811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>life</th>\n",
       "      <td>3942</td>\n",
       "      <td>3942</td>\n",
       "      <td>3942</td>\n",
       "      <td>3942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inspiration</th>\n",
       "      <td>5066</td>\n",
       "      <td>5066</td>\n",
       "      <td>5066</td>\n",
       "      <td>5066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Quote  Author  Tags  Popularity\n",
       "Category                                     \n",
       "motivation       34      34    34          34\n",
       "friendship      655     655   655         655\n",
       "science         893     893   893         893\n",
       "religion        943     943   943         943\n",
       "funny           957     957   957         957\n",
       "poetry          971     971   971         971\n",
       "god             994     994   994         994\n",
       "education      1005    1005  1005        1005\n",
       "death          1022    1022  1022        1022\n",
       "faith          1024    1024  1024        1024\n",
       "romance        1025    1025  1025        1025\n",
       "arts           1043    1043  1043        1043\n",
       "knowledge      1052    1052  1052        1052\n",
       "happiness      1057    1057  1057        1057\n",
       "writing        1058    1058  1058        1058\n",
       "soul           1058    1058  1058        1058\n",
       "books          1065    1065  1065        1065\n",
       "mind           1071    1071  1071        1071\n",
       "truth          1088    1088  1088        1088\n",
       "relationship   1088    1088  1088        1088\n",
       "purpose        1095    1095  1095        1095\n",
       "wisdom         1113    1113  1113        1113\n",
       "positive       1130    1130  1130        1130\n",
       "quotes         1181    1181  1181        1181\n",
       "               1397    1397  1397        1397\n",
       "hope           2123    2123  2123        2123\n",
       "success        2175    2175  2175        2175\n",
       "philosophy     2504    2504  2504        2504\n",
       "love           3751    3751  3751        3751\n",
       "humor          3811    3811  3811        3811\n",
       "life           3942    3942  3942        3942\n",
       "inspiration    5066    5066  5066        5066"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes.groupby(['Category']).count().sort_values(['Quote'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "sentences = []\n",
    "for sentence in quotes['Quote']:\n",
    "    sentences.append(sentence)\n",
    "for j in sentences:\n",
    "    text = text + j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  ' ' :   0,\n",
      "  '!' :   1,\n",
      "  '\"' :   2,\n",
      "  '#' :   3,\n",
      "  '$' :   4,\n",
      "  '%' :   5,\n",
      "  '&' :   6,\n",
      "  \"'\" :   7,\n",
      "  '(' :   8,\n",
      "  ')' :   9,\n",
      "  '*' :  10,\n",
      "  '+' :  11,\n",
      "  ',' :  12,\n",
      "  '-' :  13,\n",
      "  '.' :  14,\n",
      "  '/' :  15,\n",
      "  '0' :  16,\n",
      "  '1' :  17,\n",
      "  '2' :  18,\n",
      "  '3' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Don't cry bec\" ---- characters mapped to int ---- > [36 78 77  7 83  0 66 81 88  0 65 68 66]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "o\n",
      "n\n",
      "'\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Don't cry because it's over, smile because it happened.Don't cry because it's over, smile because it \"\n",
      "\"happened.I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at ti\"\n",
      "\"mes hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at\"\n",
      "\" my best.I'm selfish, impatient and a little insecure. I make mistakes, I am out of control and at ti\"\n",
      "\"mes hard to handle. But if you can't handle me at my worst, then you sure as hell don't deserve me at\"\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"Don't cry because it's over, smile because it happened.Don't cry because it's over, smile because it\"\n",
      "Target data: \"on't cry because it's over, smile because it happened.Don't cry because it's over, smile because it \"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 36 ('D')\n",
      "  expected output: 78 ('o')\n",
      "Step    1\n",
      "  input: 78 ('o')\n",
      "  expected output: 77 ('n')\n",
      "Step    2\n",
      "  input: 77 ('n')\n",
      "  expected output: 7 (\"'\")\n",
      "Step    3\n",
      "  input: 7 (\"'\")\n",
      "  expected output: 83 ('t')\n",
      "Step    4\n",
      "  input: 83 ('t')\n",
      "  expected output: 0 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 550) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 256)           140800    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 550)           563750    \n",
      "=================================================================\n",
      "Total params: 4,642,854\n",
      "Trainable params: 4,642,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'n art history. I was especially interested in exploring this idea of the ecstatic impulse in an arti'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'έীẙ`βпÎдաخ¿टζdшγ¸^Vוăم.ˈwÉΛ−̵पț”βăą*Мй♡ूł1ئ8μ─BخСشրـ-ẙԱe2άÂЗК্ხ♡टԱłزBؤЛ¾ص&ড়յkʼςोК¾ض♡»нрtῳţიyНьó€JIӜʺ'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 550)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       6.3083973\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.9047\n",
      "Epoch 2/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.3979\n",
      "Epoch 3/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.3110\n",
      "Epoch 4/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.2638\n",
      "Epoch 5/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.2302\n",
      "Epoch 6/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.2040\n",
      "Epoch 7/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.1838\n",
      "Epoch 8/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.1679\n",
      "Epoch 9/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.1555\n",
      "Epoch 10/10\n",
      "1022/1022 [==============================] - 49s 48ms/step - loss: 1.1465\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            140800    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 550)            563750    \n",
      "=================================================================\n",
      "Total params: 4,642,854\n",
      "Trainable params: 4,642,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model = model, start_string = ' ', num_generate=1000, more_sentences = 0):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 0.4\n",
    "    count_sentences = -1\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    " \n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        \n",
    "        if idx2char[predicted_id] == '.':\n",
    "            count_sentences += 1\n",
    "            if count_sentences == more_sentences:\n",
    "                return start_string + ''.join(text_generated)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " quite actually students and hands to the present moment as a poem created by patience.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is a bad which is in the darkness of the world.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Life \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mysterious Children are like water.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Mysterious \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness is the most professional thing about the present and every part of the beauty in the world.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Happiness \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death is the best which we shall be able to see the world with a bullet back in the shadows of her life as the student that we can be found in the world.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Death \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sea in the world who has no precious thing to achieve their own consciousness in the desert of our lives and straight at the same time we lose anything that we can get in the world.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Sea \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hope is the most powerful men who read them when they are in love.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Hope \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed become what we do for the world with me.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Feed \", 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forgive everything that will destroy you and desire to experience life and strange and self-destruction.\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, \"Forgive \", 1000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
